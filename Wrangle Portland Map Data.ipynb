{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766456d8",
   "metadata": {},
   "source": [
    "# Audit the Data\n",
    "\n",
    "To start, I'll import the necessary modules and parse through the data, checking for errors that need to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae2a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import unicodecsv\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import pprint\n",
    "import xml.etree.cElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f3f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 707,\n",
      " 'nd': 50170,\n",
      " 'node': 43852,\n",
      " 'osm': 1,\n",
      " 'relation': 43,\n",
      " 'tag': 32892,\n",
      " 'way': 5733}\n"
     ]
    }
   ],
   "source": [
    "# Count_tags code from the Iterative Parsing lesson of osm case study and in file 'iterative-parsing.py'.\n",
    "# This counts the amount of tags of each type.\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', )):\n",
    "        if elem.tag not in tags:\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    return tags\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('sample.osm')\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2762306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 16640, 'lower_colon': 16019, 'other': 233, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "# code taken from the 'tag types' lesson in the OSM case study, also in file \"tag types.py\"\n",
    "# This code retrieves the possible errors based on the error mapping below, so that we know what to expect.\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.match(element.attrib['k']):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    keys = process_map('sample.osm')\n",
    "    pprint.pprint(keys)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b90f4",
   "metadata": {},
   "source": [
    "Now that we have the tag types and what to expect with possible errors, lets look at the users. The code below will tell us how many unique contributing users we have in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d7893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    }
   ],
   "source": [
    "# The code below comes from the Exploring Users lesson in the OSM case study, and in file '7-explore-users.py'\n",
    "# This code will parse out how many total unique users have added to the map data. \n",
    "# It adds each unique user to the dictionary and then returns the length of the dictionary.\n",
    "\n",
    "def get_user(element):\n",
    "    uid = ''\n",
    "    if element.tag == \"node\" or element.tag == \"way\" or element.tag == \"relation\":\n",
    "        uid = element.get('uid')\n",
    "    return uid\n",
    "\n",
    "def proc_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if get_user(element):\n",
    "            users.add(get_user(element))\n",
    "            users.discard('')\n",
    "        pass\n",
    "    return users\n",
    "\n",
    "def test():\n",
    "    users = proc_map('sample.osm')\n",
    "    pprint.pprint(len(users))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6f849",
   "metadata": {},
   "source": [
    "There is invariably going to be incorrect entries into the database, and one major one may be incorrectly named street names. I'll clean those up in the sample below to better sort the data into the DB. First I'll idenitfy them, and then I'll apply the mapping to update the names into a cleaner format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f78a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# using %%capture to reduce unwanted output from the long cells\n",
    "# This code is also saved in the 'improve-street-names1.py' file.\n",
    "# This code audits the street names so that we can see what might need to be fixed.\n",
    "\n",
    "OSMFILE = \"sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Plaza\", \"Park\"]\n",
    "\n",
    "#this pulls the street data from the raw string.\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "#organizes the dictionary of values\n",
    "\n",
    "def print_sorted_dic(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print(\"%s: %d\" % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "#  audits the data by iterating through the dictionary of street types and returnng values that match the tag structure\n",
    "\n",
    "def audit():\n",
    "# osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(OSMFILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    pprint.pprint(dict(street_types))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a2874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southwest Broadway => Southwest Broadway\n",
      "Northeast Broadway => Northeast Broadway\n",
      "Southwest Burns Way => Southwest Burns Way\n",
      "Southeast Wyndham Way => Southeast Wyndham Way\n",
      "Southeast Rust Way => Southeast Rust Way\n",
      "Southwest Legacy Oak Way => Southwest Legacy Oak Way\n",
      "Woodglen Way => Woodglen Way\n",
      "Southwest 18th Way => Southwest 18th Way\n",
      "Southwest Jessica Way => Southwest Jessica Way\n",
      "Southwest Kalyca Way => Southwest Kalyca Way\n",
      "Southeast Sieben Park Way => Southeast Sieben Park Way\n",
      "Northwest Tam O' Shanter Way => Northwest Tam O' Shanter Way\n",
      "Northeast Fairview Lake Way => Northeast Fairview Lake Way\n",
      "South Big Sky Way => South Big Sky Way\n",
      "Southeast Deer Creek Way => Southeast Deer Creek Way\n",
      "Southwest Alpine Crest Way => Southwest Alpine Crest Way\n",
      "Southwest Sonnet Way => Southwest Sonnet Way\n",
      "North Burgard Way => North Burgard Way\n",
      "Orchard Way => Orchard Way\n",
      "Southeast Megan Way => Southeast Megan Way\n",
      "Southeast Vivian Way => Southeast Vivian Way\n",
      "Northeast Vista Way => Northeast Vista Way\n",
      "Southeast La Mesa Way => Southeast La Mesa Way\n",
      "Riverknoll Way => Riverknoll Way\n",
      "Royce Way => Royce Way\n",
      "Pennys Way => Pennys Way\n",
      "Southwest Carriage Way => Southwest Carriage Way\n",
      "Southeast Gladys Rose Way => Southeast Gladys Rose Way\n",
      "Blue Blossom Way => Blue Blossom Way\n",
      "Southwest Walnut Creek Way => Southwest Walnut Creek Way\n",
      "Southeast Sylvian Way => Southeast Sylvian Way\n",
      "Southwest Yearling Way => Southwest Yearling Way\n",
      "Palomino Way => Palomino Way\n",
      "Southwest Baler Way => Southwest Baler Way\n",
      "Northeast Robin Way => Northeast Robin Way\n",
      "Southwest Crow Way => Southwest Crow Way\n",
      "Southwest Galena Way => Southwest Galena Way\n",
      "North Vancouver Way => North Vancouver Way\n",
      "Ballad Way => Ballad Way\n",
      "Northeast 12th Way => Northeast 12th Way\n",
      "Southwest Frenwood Way => Southwest Frenwood Way\n",
      "Northwest Cannon Way => Northwest Cannon Way\n",
      "Southwest Meadowood Way => Southwest Meadowood Way\n",
      "Southwest Malloy Way => Southwest Malloy Way\n",
      "Southeast Tamarack Way => Southeast Tamarack Way\n",
      "North Rosa Parks Way => North Rosa Parks Way\n",
      "Carriage Way => Carriage Way\n",
      "Southwest Steele Way => Southwest Steele Street\n",
      "Southwest Herb Way => Southwest Herb Way\n",
      "Southwest Tremont Way => Southwest Tremont Way\n",
      "Southwest Teton Way => Southwest Teton Way\n",
      "Hill Way => Hill Way\n",
      "Southwest Brianne Way => Southwest Brianne Way\n",
      "Southwest Colyer Way => Southwest Colyer Way\n",
      "Southwest Kingfisher Way => Southwest Kingfisher Way\n",
      "Southwest Pattulo Way => Southwest Pattulo Way\n",
      "Northwest Westbrook Way => Northwest Westbrook Way\n",
      "Southwest Thrasher Way => Southwest Thrasher Way\n",
      "Northwest Wooded Way => Northwest Wooded Way\n",
      "Southeast Tidwells Way => Southeast Tidwells Way\n",
      "Northeast Rosa Parks Way => Northeast Rosa Parks Way\n",
      "Boones Way => Boones Way\n",
      "Harvey Way => Harvey Way\n",
      "Southeast Emily Park Way => Southeast Emily Park Way\n",
      "Southwest Apple Way => Southwest Apple Way\n",
      "Southwest Bancroft Way => Southwest Bancroft Way\n",
      "Southwest 27th Way => Southwest 27th Way\n",
      "Southwest Arborcrest Way => Southwest Arborcrest Way\n",
      "Southeast 34th Way => Southeast 34th Way\n",
      "Southwest Pathfinder Way => Southwest Pathfinder Way\n",
      "Southwest Tearose Way => Southwest Tearose Way\n",
      "Shadow Ridge Way => Shadow Ridge Way\n",
      "Southeast Garnet Way => Southeast Garnet Way\n",
      "Bellevue Way => Bellevue Way\n",
      "Southeast Verns Way => Southeast Verns Way\n",
      "Skellenger Way => Skellenger Way\n",
      "Southwest Tallac Way => Southwest Tallac Way\n",
      "Southeast Valley Way => Southeast Valley Way\n",
      "Southeast Sigs Way => Southeast Sigs Way\n",
      "Southwest Bristlecone Way => Southwest Bristlecone Way\n",
      "Northeast Lija Loop => Northeast Lija Loop\n",
      "Southwest Fanno Creek Loop => Southwest Fanno Creek Loop\n",
      "Southwest 184th Loop => Southwest 184th Loop\n",
      "Southwest Fallatin Loop => Southwest Fallatin Loop\n",
      "Southeast 12th Loop => Southeast 12th Loop\n",
      "Northwest Wood Rose Loop => Northwest Wood Rose Loop\n",
      "Southeast Holland Loop => Southeast Holland Loop\n",
      "Southwest Sandlewood Loop => Southwest Sandlewood Loop\n",
      "Southeast Dunhill Loop => Southeast Dunhill Loop\n",
      "Castleberry Loop => Castleberry Loop\n",
      "Southeast Nicole Loop => Southeast Nicole Loop\n",
      "Southwest Ivory Loop => Southwest Ivory Loop\n",
      "Southwest Verdun Loop => Southwest Verdun Loop\n",
      "Southeast Summerfield Loop => Southeast Summerfield Loop\n",
      "Northwest Forestel Loop => Northwest Forestel Loop\n",
      "Southwest Calusa Loop => Southwest Calusa Loop\n",
      "Southeast Highland Loop => Southeast Highland Loop\n",
      "Southwest Copper Creek Loop => Southwest Copper Creek Loop\n",
      "Southeast Osprey Loop => Southeast Osprey Loop\n",
      "Southeast Reserve Loop => Southeast Reserve Loop\n",
      "Southeast Fir Grove Loop => Southeast Fir Grove Loop\n",
      "Southeast Pinegrove Loop => Southeast Pinegrove Loop\n",
      "Southwest Kraft Loop => Southwest Kraft Loop\n",
      "Southeast Augusta Loop => Southeast Augusta Loop\n",
      "Southeast Emerald Loop => Southeast Emerald Loop\n",
      "Southwest 38th Loop => Southwest 38th Loop\n",
      "Southwest Beaverton Hillsdale Highway => Southwest Beaverton Hillsdale Highway\n",
      "Southeast Tualatin Valley Highway => Southeast Tualatin Valley Highway\n",
      "Southwest Hillsboro Highway => Southwest Hillsboro Highway\n",
      "East Historic Columbia River Highway => East Historic Columbia River Highway\n",
      "West Historic Columbia River Highway => West Historic Columbia River Highway\n",
      "Creekside Terrace => Creekside Terrace\n",
      "Southwest Tephra Terrace => Southwest Tephra Terrace\n",
      "Southwest Tanager Terrace => Southwest Tanager Terrace\n",
      "Windsor Terrace => Windsor Terrace\n",
      "Northwest Lansbrook Terrace => Northwest Lansbrook Terrace\n",
      "Southwest 149th Terrace => Southwest 149th Terrace\n",
      "Northeast Fleming Terrace => Northeast Fleming Terrace\n",
      "Southwest Sire Terrace => Southwest Sire Terrace\n",
      "Southwest Hallmark Terrace => Southwest Hallmark Terrace\n",
      "North Overlook Terrace => North Overlook Terrace\n",
      "Woodland Terrace => Woodland Terrace\n",
      "Northwest Fairfax Terrace => Northwest Fairfax Terrace\n",
      "Cascade Terrace => Cascade Terrace\n",
      "Southwest Clydesdale Terrace => Southwest Clydesdale Terrace\n",
      "Southwest Tranquility Terrace => Southwest Tranquility Terrace\n",
      "Kafton Terrace => Kafton Terrace\n",
      "Southwest 68th Terrace => Southwest 68th Terrace\n",
      "Northwest Lark Meadow Terrace => Northwest Lark Meadow Terrace\n",
      "Northwest 164th Terrace => Northwest 164th Terrace\n",
      "Southwest 28th Terrace => Southwest 28th Terrace\n",
      "Southwest View Point Terrace => Southwest View Point Terrace\n",
      "Southwest Cascade Terrace => Southwest Cascade Terrace\n",
      "Southwest Millennium Terrace => Southwest Millennium Terrace\n",
      "Southwest Metta Terrace => Southwest Metta Terrace\n",
      "Southwest Trapper Terrace => Southwest Trapper Terrace\n",
      "Southwest 205th Terrace => Southwest 205th Terrace\n",
      "Sunburst Terrace => Sunburst Terrace\n",
      "Southeast 15th Terrace => Southeast 15th Terrace\n",
      "Southeast Valley View Terrace => Southeast Valley View Terrace\n",
      "Northwest Monte Vista Terrace => Northwest Monte Vista Terrace\n",
      "Southwest Othello Terrace => Southwest Othello Terrace\n",
      "Northwest Byrne Terrace => Northwest Byrne Terrace\n",
      "Southwest Mercer Terrace => Southwest Mercer Terrace\n",
      "Southwest Rask Terrace => Southwest Rask Terrace\n",
      "Northwest Mcgregor Terrace => Northwest Mcgregor Terrace\n",
      "Southwest Windrose Terrace => Southwest Windrose Terrace\n",
      "Southeast 29th Terrace => Southeast 29th Terrace\n",
      "Southwest 176th Terrace => Southwest 176th Terrace\n",
      "Southwest Old Highway 47 => Southwest Old Highway 47\n",
      "Overlook Circle => Overlook Circle\n",
      "Southeast 16th Circle => Southeast 16th Circle\n",
      "Northeast Gertz Circle => Northeast Gertz Circle\n",
      "Southwest Emery Circle => Southwest Emery Circle\n",
      "Wake Robin Circle => Wake Robin Circle\n",
      "Maple Circle => Maple Circle\n",
      "Partridge Circle => Partridge Circle\n",
      "Southwest 19th Circle => Southwest 19th Circle\n",
      "Northeast 75th Circle => Northeast 75th Circle\n",
      "Southwest Richen Park Circle => Southwest Richen Park Circle\n",
      "Southeast Quail Circle => Southeast Quail Circle\n",
      "Southwest Bobwhite Circle => Southwest Bobwhite Circle\n",
      "Southwest Steeplechase Circle => Southwest Steeplechase Street\n",
      "Southwest Briarcliff Circle => Southwest Briarcliff Circle\n",
      "Canal Circle => Canal Circle\n",
      "Southwest Dale Circle => Southwest Dale Circle\n",
      "Southeast Norma Circle => Southeast Norma Circle\n",
      "Southwest Stone Ridge Circle => Southwest Stone Ridge Street\n",
      "Northeast Laurelwood Circle => Northeast Laurelwood Circle\n",
      "Long Circle => Long Circle\n",
      "Southwest 12th Circle => Southwest 12th Circle\n",
      "Morningview Circle => Morningview Circle\n",
      "Indian Springs Circle => Indian Springs Circle\n",
      "Southwest Alderbrook Circle => Southwest Alderbrook Circle\n",
      "Southwest Glacier Lily Circle => Southwest Glacier Lily Circle\n",
      "Bland Circle => Bland Circle\n",
      "Southeast Ankeny Circle => Southeast Ankeny Circle\n",
      "Amberwood Circle => Amberwood Circle\n",
      "South Highway 99E => South Highway 99E\n",
      "Collins Crest => Collins Crest\n",
      "Churchill Downs => Churchill Downs\n",
      "Horseshoe Curve => Horseshoe Curve\n",
      "Southwest Coleman Loop North => Southwest Coleman Loop North\n",
      "Southwest Jamaica => Southwest Jamaica\n",
      "Country Commons => Country Commons\n",
      "Southeast Highway 224 => Southeast Highway 224\n",
      "Southeast Highway 212 => Southeast Highway 212\n",
      "Highway 211 => Highway 211\n",
      "Southwest Costa Circle West => Southwest Costa Circle West\n",
      "Highway 26 => Highway 26\n"
     ]
    }
   ],
   "source": [
    "# This code is also saved in the 'improve-street-names2.py' file.\n",
    "\n",
    "mapping = {\"St\": \"Street\",\n",
    "           \"ST\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"St,\": \"Street\",\n",
    "           \"Street.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Sq\": \"Square\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"DR.\": \"Drive\"\n",
    "           }\n",
    "\n",
    "# audits the code again to check it against the expected values.\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "# updates the errors against the mapping to replace the name with the correct form.\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for key, value in mapping.items():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(street_type_re, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b2547",
   "metadata": {},
   "source": [
    "The main element fixes are the tags for Streets and the State, where incorrect data might be placed that would remove large aspects of the city from my analysis. Those will be the focus before exporting. While some streets could be wholly incorrect, in this clean-up we'll just be making the names uniform for their tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21d6332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#located in 'is-state.py'\n",
    "# Audits the state element to see if it shows up as 'OR' and if its doesn't, then replaces it with 'OR'.\n",
    "\n",
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "state_types = defaultdict(int)\n",
    "\n",
    "def audit_state(state_types, state_name):\n",
    "    if state_name != 'OR':\n",
    "        state_types[state_name] += 1\n",
    "\n",
    "for event, elem in ET.iterparse(OSMFILE, events =(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_state(tag):\n",
    "                audit_state(state_types, tag.attrib['v'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42778e0",
   "metadata": {},
   "source": [
    "Just a few mislabeled state elements as WA instead of OR. There is likely many more due to the sheer size of the original extract. Far too many to fix here. But these can be replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca6733",
   "metadata": {},
   "source": [
    "## Reshaping elements and exporting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360f3ae",
   "metadata": {},
   "source": [
    "With the street and state element functions set, now it's time to run the process_map function in the main code, and parse out the csv files I need for the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd54345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contained in file 'schema.py' as well as lesson 11 'Case Study Open Street Map data'\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605e274e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street Names and State Codes Fixed:\n",
      "\n",
      "\n",
      "Fixed State:  WA => 'OR'\n",
      "Fixed State:  WA => 'OR'\n",
      "Fixed Street: NE 22nd Ave => NE 22nd Avenue\n",
      "Fixed State:  WA => 'OR'\n",
      "Fixed State:  WA => 'OR'\n",
      "Fixed State:  WA => 'OR'\n",
      "Reshaping and export complete.\n"
     ]
    }
   ],
   "source": [
    "#original code in the 'data.py' file also in the lesson 11 of 'Case Study OpenStreetMap data'\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import unicodecsv\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "# import cerberus\n",
    "# Cerberus commented out because Anaconda environment refuses to accept cerberus package. \n",
    "# All validator code commented out because of this environment issue. \n",
    "# The rest of the code can still function without cerberus validator.\n",
    "\n",
    "# import schema\n",
    "\n",
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "print (\"Street Names and State Codes Fixed:\")\n",
    "print (\"\\n\")\n",
    "\n",
    "# Fix Street & State Names \n",
    "\n",
    "#located in 'fix-street-names.py'\n",
    "\n",
    "OSMFILE = \"sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Plaza\", \"Park\"]\n",
    "\n",
    "mapping = {\"St\": \"Street\",\n",
    "           \"ST\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"St,\": \"Street\",\n",
    "           \"Street.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Sq\": \"Square\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"DR.\": \"Drive\",\n",
    "           \"Blvd\": \"Boulevard\"\n",
    "           }\n",
    "\n",
    "# Auditing the street names and creating the dictionary for teh fixing element to iterate through.\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "# updates the errors against the mapping to replace the name with the correct form.\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for key, value in mapping.items():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(street_type_re, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "\n",
    "            \n",
    "# Auditing state functions to parse through for the fixing function\n",
    "\n",
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "state_types = defaultdict(int)\n",
    "\n",
    "def audit_state(state_types, state_name):\n",
    "    if state_name != 'OR':\n",
    "        state_types[state_name] += 1\n",
    "\n",
    "for event, elem in ET.iterparse(OSMFILE, events =(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_state(tag):\n",
    "                audit_state(state_types, tag.attrib['v'])\n",
    "\n",
    "\n",
    "# fixes the street names by checking if its in the mapping and then replacing the errors to be uniform accrodingly\n",
    "\n",
    "def fix_street(elem):\n",
    "\n",
    "    street_types = defaultdict(set)\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_street_name(tag):\n",
    "                audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "            for st_type, ways in street_types.items():\n",
    "                for name in ways:\n",
    "                    for key,value in mapping.items():\n",
    "                        n = street_type_re.search(name)\n",
    "                        if n:\n",
    "                            street_type = n.group()\n",
    "                            if street_type not in expected:\n",
    "                                if street_type in mapping:\n",
    "                                    better_name = name.replace(key,value)\n",
    "                                    if better_name != name:\n",
    "                                        print (\"Fixed Street:\", tag.attrib['v'], \"=>\", better_name)\n",
    "                                        tag.attrib['v'] = better_name\n",
    "                                        return\n",
    "#is located in 'fix-state.py'\n",
    "#This will replace the incorrect state (WA) with (OR).\n",
    "                                    \n",
    "def fix_state(elem):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_state(tag):\n",
    "                    if tag.attrib['v'] != 'OR':\n",
    "                        print (\"Fixed State: \", tag.attrib['v'], \"=> 'OR'\")\n",
    "                        tag.attrib['v'] = 'OR'\n",
    "                                    \n",
    "def fix_element(elem):\n",
    "    fix_street(elem)\n",
    "    fix_state(elem)\n",
    "                                \n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # Fix data issues, based on auditing results\n",
    "    # runs the fix_element function which calls the fix state and fix street functions. \n",
    "    # This will replace the necessary errors.\n",
    "    # The rest of shape_element will then run through the nodes and ways tags to make sure that all\n",
    "    # of the elements are correct type for each row location.\n",
    "    \n",
    "    fix_element(element)\n",
    "\n",
    "    if element.tag == 'node':\n",
    "\n",
    "            for node_field in node_attr_fields:\n",
    "                node_attribs[node_field] =element.attrib[node_field]\n",
    "\n",
    "            for tag in element.iter('tag'):\n",
    "                k = tag.attrib['k']\n",
    "\n",
    "                # ignores tags containing problem characters in the k tag attribute:\n",
    "\n",
    "                if re.search(PROBLEMCHARS,k):\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                tag_dict = {}\n",
    "\n",
    "                tag_dict['id'] = node_attribs['id']\n",
    "\n",
    "                colon_find = re.split('[:]', k)\n",
    "\n",
    "                if len(colon_find) == 1:\n",
    "\n",
    "                    tag_dict['key'] = k\n",
    "                    tag_dict['type'] = 'regular'\n",
    "\n",
    "                elif len(colon_find) == 2:\n",
    "\n",
    "                    tag_dict['key'] = colon_find[1]\n",
    "                    tag_dict['type'] = colon_find[0]\n",
    "\n",
    "                elif len(colon_find) > 2:\n",
    "\n",
    "                    tag_dict['key'] = ':'.join(colon_find[1:])\n",
    "                    tag_dict['type'] = colon_find[0]\n",
    "\n",
    "                tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "                tags.append(tag_dict)\n",
    "\n",
    "            return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "\n",
    "        for way_field in way_attr_fields:\n",
    "            way_attribs[way_field] =element.attrib[way_field]\n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.attrib['k']\n",
    "\n",
    "            # ignores tags containing problem characters in the k tag attribute:\n",
    "\n",
    "            if re.search(PROBLEMCHARS,k):\n",
    "                print (\"Problem character found - skipping element\")\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            tag_dict = {}\n",
    "\n",
    "            tag_dict['id'] = way_attribs['id']\n",
    "\n",
    "            colon_find = re.split('[:]', k)\n",
    "\n",
    "            if len(colon_find) == 1:\n",
    "\n",
    "                tag_dict['key'] = k\n",
    "                tag_dict['type'] = 'regular'\n",
    "\n",
    "            elif len(colon_find) == 2:\n",
    "\n",
    "                tag_dict['key'] = colon_find[1]\n",
    "                tag_dict['type'] = colon_find[0]\n",
    "\n",
    "            elif len(colon_find) > 2:\n",
    "\n",
    "                tag_dict['key'] = ':'.join(colon_find[1:])\n",
    "                tag_dict['type'] = colon_find[0]\n",
    "\n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "            tags.append(tag_dict)\n",
    "\n",
    "        n = 0\n",
    "        for nd in element.iter('nd'):\n",
    "\n",
    "            nd_dict = {}\n",
    "\n",
    "            nd_dict['id'] = way_attribs['id']\n",
    "            nd_dict['node_id'] = nd.attrib['ref']\n",
    "            nd_dict['position'] = n\n",
    "            way_nodes.append(nd_dict)\n",
    "            n+=1\n",
    "\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "        \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "# Validator elements commented out because cerberus package can't be installed. See above comments.\n",
    "\n",
    "#def validate_element(element, validator, schema=schema):\n",
    "#    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "#    if validator.validate(element, schema) is not True:\n",
    "#        field, errors = next(validator.errors.items())\n",
    "#        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "#        error_string = pprint.pformat(errors)\n",
    "#        \n",
    "#        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow(\n",
    "            {\n",
    "                k: (v.encode(\"utf-8\") if isinstance(v, str) else v)\n",
    "                for k, v in row.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w', 'utf-8') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w', 'utf-8') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w', 'utf-8') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w', 'utf-8') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w', 'utf-8') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "    # Validator elements commented out because cerberus package can't be installed. See above comments.\n",
    "        # validator = cerberus.Validator()\n",
    "    \n",
    "    # process_map() relies on shape_element() to iterate thru the code and parse which dict the data is parsed to\n",
    "    # and then it writes it to the appropriate CSV file.\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "              #  if validate is True:\n",
    "               #     validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n",
    "    \n",
    "print(\"Reshaping and export complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f72714",
   "metadata": {},
   "source": [
    "Now I need to import the database and get it set up successfully.\n",
    "\n",
    "Since I hade some module issues with getting Anaconda to accept certain packages, I've taken the CSV files written and imported them to Microsoft Access to perform the queries there. I'll provide a graphic representation below of each of the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f171c",
   "metadata": {},
   "source": [
    "### Size of the files\n",
    "\n",
    "| File      | Size |\n",
    "| ----------- | ----------- |\n",
    "| Portland_oregonosm.xml      | 1.46 gb       |\n",
    "| Sample.osm   |   10.2 mb      |\n",
    "| Portland.accdb  |  14 mb  |\n",
    "| nodes.csv  |  3.9 mb  |\n",
    "| nodes_tags.csv  |  65 kb  |\n",
    "| ways.csv   |  388 kb  |\n",
    "| ways_tags.csv  |  1 mb  |\n",
    "| ways_nodes.csv  |  1.1 mb  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d0046",
   "metadata": {},
   "source": [
    "### The number of nodes\n",
    "\n",
    ">SELECT COUNT(*) FROM nodes;\n",
    "\n",
    "43852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a70b3b",
   "metadata": {},
   "source": [
    "### The number of ways\n",
    "\n",
    ">SELECT COUNT(*) FROM ways;\n",
    "\n",
    "5733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864953c8",
   "metadata": {},
   "source": [
    "### Unique Users\n",
    "\n",
    "It may be noticeable that the SQL is not regular sqlite SQL but the syntax necessary for Microsoft Access.\n",
    "\n",
    ">SELECT COUNT(*)  \n",
    ">FROM (SELECT DISTINCT uid FROM nodes UNION ALL SELECT DISTINCT uid FROM ways) e;\n",
    "\n",
    "461"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5167fb",
   "metadata": {},
   "source": [
    "### Top 10 users\n",
    "\n",
    "SELECT TOP 10 user, COUNT(\\*) FROM  \n",
    "(SELECT user FROM nodes UNION ALL SELECT user FROM ways)  \n",
    "GROUP BY user  \n",
    "ORDER BY COUNT(\\*) DESC;  \n",
    "\n",
    "| user\t| Count  |\n",
    "| ----------- | ----------- |\n",
    "|  Peter Dobratz_pdxbuildings  |  13033  |\n",
    "|  lyzidiamond_imports  |  12689  |\n",
    "|  Mele Sax-Barnett  |  3843 |\n",
    "| baradam  |  \t3343  |\n",
    "| Darrell_pdxbuildings  | \t2863  |\n",
    "| cowdog  |  \t2182  |\n",
    "| Peter Dobratz  |  \t2008  |\n",
    "| Grant Humphries  |  \t1937  | \n",
    "| justin_pdxbuildings  |  \t776  |\n",
    "| amillar-osm-import  |  \t725  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca474c6",
   "metadata": {},
   "source": [
    "### Amenity with most entries\n",
    "\n",
    "SELECT TOP 1 value, COUNT(\\*)  \n",
    "FROM nodes_tags  \n",
    "WHERE key=\"amenity\"  \n",
    "GROUP BY value  \n",
    "ORDER BY COUNT(\\*) DESC;  \n",
    "\n",
    "|  Value  |  Count  |\n",
    "| -------- | ------- | \n",
    "|  bicycle_parking  |  13   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87477997",
   "metadata": {},
   "source": [
    "Let's do a silly one. This is the Pacific Northwest, so lets see how many Starbucks there are in the entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be49191",
   "metadata": {},
   "source": [
    "### Number of Starbucks\n",
    "\n",
    "SELECT count (*)  \n",
    "FROM nodes_tags  \n",
    "WHERE value ='STARBUCKS';  \n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6b828",
   "metadata": {},
   "source": [
    "## Problems in the Data and Suggestions\n",
    "\n",
    "Auditing the sample size of Portland map, a few problems came up that could be fixed and cleaned early on, before adding to the database. Abbreviated street names and incorrect state listings were the most common.\n",
    "This was fixed programmatically above using a few functions.\n",
    "\n",
    "Now that the data is in the database, it is clear there are a few other problems in the data, which interferes with making deeper query assertations. Across the three 'nodes' tables, there are a significant amount of missing 'id' entries, which shrinks the amount of outputs for joined queries considerably. This could bring the numbers/COUNT outputs into question, as some rows may get excepted when querying nodes JOIN nodes_tags queries.\n",
    "\n",
    "One of the key problems with datasets this large, even for just a relatively small sample of a single city, is the amount of unique points being logged. A unique id for each node tag quickly scales into the billions. Rather than an overall unique id for each tag, it would be helpful to subset the id for tags to each user and type, reducing the overall numeric scale for each tag. Even would simply require more specificty in querying as the unique user would need to be a query element as well as the id's, to differentiate if any numbers duplicated (which they would)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edca92",
   "metadata": {},
   "source": [
    "### Additional exploration\n",
    "\n",
    "Below is an extra query to explore some aspects of the map, like the starbucks exploration above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f209b",
   "metadata": {},
   "source": [
    "Number of religious locations:\n",
    "\n",
    "SELECT value, COUNT(\\*)  \n",
    "FROM nodes_tags WHERE key=\"religion\"  \n",
    "GROUP BY value  \n",
    "ORDER BY COUNT(\\*) DESC;  \n",
    "\n",
    "|  Value  |  Count  |\n",
    "| ------- | ---------- |\n",
    "| christian  |  2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54d13a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It's apparent even from my sample that the Portland extract of data is positively massive, and it would have been much easier to parse through with a smaller city scale. \n",
    "The most distorting factor in the data appears to be the inclusion of quite a bit of Vancouver, WA data as well, with streets and places holding WA tags. \n",
    "The data is clearly bot-submitted on many users accounts, with tag designations by the highest contributors scaling into the billions range. It would be helpful to make these designations more specific for each element, which would allow for more clean parsing when identifying problems. \n",
    "\n",
    "Many of these problems and solutions are beyond my ability to tackle, but I think it makes sense to try to fix them from an engineering perspective, to have cleaner data going into the map initially. This way there is less cleaning necessary on the back end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217b056",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Street names code confirmation:\n",
    "https://github.com/belgarion42/c750/blob/master/Udacity%20OSM%20Project%20-%20Irvine%2C%20CA%20-%20Randy%20Crane.ipynb\n",
    "https://github.com/wblakecannon/udacity-dand/blob/master/4-Data-Wrangling/L12-Case-Study-OpenStreetMap-Data/11-Quiz-Preparing-for-Database-SQL.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
